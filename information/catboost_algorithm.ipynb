{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CatBoost is a gradient boosting algorithm that uses categorical features more effectively than many other algorithms. It is particularly well-suited for datasets with categorical features. Here’s a detailed explanation of how CatBoost works, its differences from other classification algorithms, and an example of how to use it for your credit risk management problem.\n",
    "\n",
    "### How CatBoostClassifier Works\n",
    "\n",
    "1. **Gradient Boosting**: CatBoost, like other boosting algorithms, builds an ensemble of decision trees where each subsequent tree attempts to correct the errors of the previous trees.\n",
    "2. **Categorical Features**: CatBoost can handle categorical features natively without the need for explicit encoding like one-hot or label encoding. It uses a technique called \"Ordered Target Statistics\" to encode categorical features.\n",
    "3. **Ordered Target Statistics**: CatBoost processes categorical features by converting them into numerical features using target statistics calculated in an ordered manner to avoid target leakage.\n",
    "4. **Symmetric Trees**: CatBoost builds symmetric trees where the structure of the tree is the same for all splits. This reduces the model's complexity and makes it faster and more memory efficient.\n",
    "5. **Oblivious Trees**: CatBoost uses oblivious trees (a type of symmetric trees) where each node applies the same split criterion, reducing overfitting and improving generalization.\n",
    "\n",
    "### Differences with Other Classification Algorithms\n",
    "\n",
    "1. **Handling of Categorical Features**: Unlike XGBoost and LightGBM, which require preprocessing of categorical features, CatBoost handles them natively.\n",
    "2. **Symmetric Trees**: CatBoost's symmetric trees differ from the asymmetric trees used by other algorithms, leading to faster training and prediction.\n",
    "3. **Training Speed**: CatBoost can be faster than other gradient boosting algorithms due to its efficient handling of categorical features and tree structure.\n",
    "4. **Accuracy**: CatBoost often achieves higher accuracy on datasets with many categorical features due to its advanced handling of these features.\n",
    "5. **Overfitting Prevention**: CatBoost incorporates several mechanisms to prevent overfitting, such as ordered boosting and permutation-driven leaf estimation.\n",
    "\n",
    "### Example Using CatBoostClassifier\n",
    "\n",
    "Here’s how you can use CatBoostClassifier for your credit risk management problem:\n",
    "\n",
    "1. **Install CatBoost**: If you haven't installed CatBoost yet, you can install it using pip.\n",
    "\n",
    "   ```bash\n",
    "   pip install catboost\n",
    "   ```\n",
    "\n",
    "2. **Prepare Your Data**: Ensure your data is in a suitable format. CatBoost can handle categorical features directly.\n",
    "\n",
    "   ```python\n",
    "   import pandas as pd\n",
    "   from catboost import CatBoostClassifier, Pool\n",
    "   from sklearn.model_selection import train_test_split\n",
    "   from sklearn.metrics import roc_auc_score\n",
    "\n",
    "   # Load your data\n",
    "   # Assuming full_df is your dataframe with the necessary features and 'flag' is the target variable\n",
    "   x = full_df.drop(columns=['id', 'flag'])\n",
    "   y = full_df['flag']\n",
    "\n",
    "   # Split the data\n",
    "   x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=1)\n",
    "\n",
    "   # Identify categorical features\n",
    "   categorical_features = [i for i, col in enumerate(x_train.columns) if x_train[col].dtype == 'object']\n",
    "   ```\n",
    "\n",
    "3. **Train the CatBoostClassifier**:\n",
    "\n",
    "   ```python\n",
    "   # Create Pool objects for CatBoost\n",
    "   train_pool = Pool(data=x_train, label=y_train, cat_features=categorical_features)\n",
    "   test_pool = Pool(data=x_test, label=y_test, cat_features=categorical_features)\n",
    "\n",
    "   # Initialize and train the model\n",
    "   catboost_model = CatBoostClassifier(\n",
    "       iterations=1000,\n",
    "       learning_rate=0.1,\n",
    "       depth=6,\n",
    "       eval_metric='AUC',\n",
    "       random_seed=1,\n",
    "       logging_level='Verbose',\n",
    "       allow_writing_files=False\n",
    "   )\n",
    "\n",
    "   catboost_model.fit(train_pool, eval_set=test_pool, early_stopping_rounds=100)\n",
    "   ```\n",
    "\n",
    "4. **Evaluate the Model**:\n",
    "\n",
    "   ```python\n",
    "   # Predict probabilities\n",
    "   train_predictions = catboost_model.predict_proba(train_pool)[:, 1]\n",
    "   test_predictions = catboost_model.predict_proba(test_pool)[:, 1]\n",
    "\n",
    "   # Calculate ROC AUC\n",
    "   train_auc = roc_auc_score(y_train, train_predictions)\n",
    "   test_auc = roc_auc_score(y_test, test_predictions)\n",
    "\n",
    "   print(f\"The ROC AUC score of CatBoostClassifier (Train dataset): {train_auc}\")\n",
    "   print(f\"The ROC AUC score of CatBoostClassifier (Test dataset): {test_auc}\")\n",
    "   ```\n",
    "\n",
    "### Summary\n",
    "CatBoost is a powerful and efficient algorithm, especially for datasets with categorical features. It provides several advantages over other gradient boosting algorithms, including better handling of categorical features, faster training times, and often higher accuracy. By following the example provided, you can leverage CatBoost for your credit risk management problem and potentially improve your model's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define batch size\n",
    "batch_size = 10000  # Adjust based on memory constraints\n",
    "\n",
    "# List to hold processed batches\n",
    "ohe_data_list = []\n",
    "\n",
    "# Process data in batches\n",
    "for start in range(0, len(df), batch_size):\n",
    "    end = start + batch_size\n",
    "    batch = df[ohe_cols].iloc[start:end]\n",
    "    ohe_data_batch = ohe.fit_transform(batch)\n",
    "    ohe_data_list.append(ohe_data_batch)\n",
    "\n",
    "# Concatenate all batches\n",
    "ohe_data = pd.DataFrame(np.concatenate(ohe_data_list, axis=0), columns=ohe.get_feature_names_out(ohe_cols))\n",
    "\n",
    "# Join the encoded data back to the original DataFrame\n",
    "df = df.reset_index(drop=True).join(ohe_data).drop(columns=ohe_cols)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
